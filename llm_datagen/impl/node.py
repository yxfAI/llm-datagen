"""èŠ‚ç‚¹å®ç°ï¼šæä¾›æ ¸å¿ƒæ‰¹å¤„ç†å®¹å™¨ä¸ç»Ÿä¸€é€‰å‹ç­–ç•¥å…¥å£"""

import threading
import time
import logging
import os
from typing import Any, List, Optional, Dict, Callable
from concurrent.futures import ThreadPoolExecutor

from llm_datagen.core.node import INode, IBatchNode, IRecoverableNode, NodeStatus
from llm_datagen.core.operators import IOperator, ISingleOperator, IBatchOperator
from llm_datagen.core.config import WriterConfig

# ------------------- èŠ‚ç‚¹çº§ Logger é…ç½® -------------------
_node_logger = logging.getLogger("DataGen.Node")

class NodeContextImpl:
    """èŠ‚ç‚¹æ‰§è¡Œä¸Šä¸‹æ–‡å®ç°"""
    def __init__(
        self,
        node_id: str,
        context_id: str,
        on_progress: Callable[[int, Optional[int], Optional[Dict]], None],
        on_usage: Callable[[Dict[str, Any]], None],
        on_log: Callable[[str, str], None],
        on_error: Callable[[Exception, List[Any]], None],
        is_cancelled_func: Callable[[], bool],
        save_checkpoint_func: Optional[Callable[[], None]] = None
    ):
        self._node_id = node_id
        self._context_id = context_id
        self._on_progress = on_progress
        self._on_usage = on_usage
        self._on_log = on_log
        self._on_error = on_error
        self._is_cancelled_func = is_cancelled_func
        self._save_checkpoint_func = save_checkpoint_func
        
        self.metrics = {"cost": 0.0, "tokens": 0}
        self.current_progress = 0
        self.total_progress: Optional[int] = 0

    @property
    def node_id(self) -> str: return self._node_id
    @property
    def context_id(self) -> str: return self._context_id
    def is_cancelled(self) -> bool: return self._is_cancelled_func()
    def report_progress(self, current: int, total: Optional[int], metadata: Optional[Dict] = None):
        self.current_progress = current; self.total_progress = total
        self._on_progress(current, total, metadata)
    def save_checkpoint(self):
        if self._save_checkpoint_func: self._save_checkpoint_func()
    def report_usage(self, metrics: Dict[str, Any]):
        for k, v in metrics.items():
            if isinstance(v, (int, float)): self.metrics[k] = self.metrics.get(k, 0) + v
        # æ ¸å¿ƒä¿®å¤ï¼šç¡®ä¿è½¬å‘ç»™å¤–éƒ¨ç›‘å¬å™¨ (Hooks)
        self._on_usage(metrics)
    def log(self, message: str, level: str = "info"): self._on_log(message, level)
    def report_failed_items(self, items: List[Any], error: Exception): self._on_error(error, items)


# ==============================================================================
# 1. BaseNode: åªç®¡æ‹“æ‰‘å’Œç”Ÿå‘½å‘¨æœŸ
# ==============================================================================

class BaseNode(IRecoverableNode):
    """
    èŠ‚ç‚¹æ‰§è¡ŒåŸºç±»ï¼š
    1. æŒæœ‰æ‹“æ‰‘é…ç½® (node_id, input_uri, output_uri)
    2. ç®¡ç†ç”Ÿå‘½å‘¨æœŸä¸ I/O ç»‘å®š
    """
    def __init__(self, 
                 node_id: str = None,
                 input_uri: Optional[str] = None,
                 output_uri: Optional[str] = None,
                 protocol_prefix: str = "",
                 base_path: str = ""):
        # æ‹“æ‰‘å±æ€§
        self._node_id = node_id
        self._input_uri = input_uri
        self._output_uri = output_uri
        self._protocol_prefix = protocol_prefix
        self._base_path = base_path
        
        # çŠ¶æ€å±æ€§
        self._status = NodeStatus.PENDING
        self._ctx: Optional[NodeContextImpl] = None
        self._input_stream = None
        self._output_stream = None
        self._current_progress = 0
        self._total_progress = 0
        self._resume_progress = None
        self._shutdown_event = threading.Event()
        self._start_time = 0
        self._end_time = 0
        self._writer_config: Optional[WriterConfig] = None

    @property
    def node_id(self) -> str: return self._node_id
    @property
    def input_uri(self) -> Optional[str]: return self._input_uri
    @input_uri.setter
    def input_uri(self, v): self._input_uri = v
    @property
    def output_uri(self) -> Optional[str]: return self._output_uri
    @output_uri.setter
    def output_uri(self, v): self._output_uri = v
    
    @property
    def log_id(self) -> str:
        cid = self._ctx.context_id if self._ctx else "init"
        return f"[{cid}:{self._node_id}]"
    @property
    def status(self) -> NodeStatus: return self._status
    @status.setter
    def status(self, value: NodeStatus): self._status = value
    @property
    def progress(self) -> Dict[str, int]: return {"current": self._current_progress, "total": self._total_progress}

    @property
    def input_stream(self): 
        return self._impl.input_stream if (self._impl and hasattr(self._impl, 'input_stream')) else self._input_stream
    @property
    def output_stream(self): 
        return self._impl.output_stream if (self._impl and hasattr(self._impl, 'output_stream')) else self._output_stream

    def bind_io(self, input_stream: Any, output_stream: Any):
        self._input_stream = input_stream
        self._output_stream = output_stream

    def set_context(self, ctx: NodeContextImpl):
        self._ctx = ctx

    def set_writer_config(self, config: WriterConfig):
        self._writer_config = config

    def open(self, ctx: Optional[NodeContextImpl] = None, progress: Optional[Any] = None):
        if ctx: self.set_context(ctx)
        _node_logger.info(f"{self.log_id} ğŸ¬ èŠ‚ç‚¹å¼€å¯ (Status={self._status.value})")
        
        # æ ¸å¿ƒä¿®å¤ï¼šå°Šé‡æ¢å¤çŠ¶æ€ï¼Œä¸è¦ç›²ç›®è¦†ç›–ä¸º RUNNING
        if self._status != NodeStatus.COMPLETED:
            self._status = NodeStatus.RUNNING
            
        self._start_time = time.time()
        
        target_progress = progress if progress is not None else self._resume_progress
        
        if self._input_stream: self._input_stream.open()
        if self._output_stream: 
            # æ ¸å¿ƒä¿®å¤ï¼šå¦‚æœæ˜¯æ¢å¤æ¨¡å¼ä¸”èŠ‚ç‚¹å°šæœªå®Œæˆï¼Œå¿…é¡»æ’¤é”€ä¹‹å‰çš„ç‰©ç†å°æ¡
            # å¦åˆ™ä¸‹æ¸¸èŠ‚ç‚¹ä¼šè¯¯ä»¥ä¸ºè¯¥æµå·²ç»“æŸ
            if self._status != NodeStatus.COMPLETED:
                if hasattr(self._output_stream, "unseal"):
                    self._output_stream.unseal()
            self._output_stream.open()
        
        self._reader = self._input_stream.get_reader(progress=target_progress) if self._input_stream else None
        self._writer = self._output_stream.get_writer(options=self._writer_config) if self._output_stream else None
        
        if self._reader:
            # æ ¸å¿ƒæ”¹è¿›ï¼šåˆå§‹æ€»é‡å¯¹é½ç‰©ç†å­˜å‚¨ï¼Œä½†è¿›åº¦ä¿æŒç”±å¿«ç…§æ³¨å…¥çš„çœŸç›¸ä½ç‚¹
            self._total_progress = self._reader.total_count
            
            _node_logger.info(f"{self.log_id} ğŸ“Š ç‰©ç†ä½ç‚¹å¯¹é½: è¿›åº¦={self._current_progress}, æ€»é‡={self._total_progress}")
            
            # æ ¸å¿ƒæ”¹è¿›ï¼šå¼€å¯æ—¶ç«‹å³ä¸ŠæŠ¥ä¸€æ¬¡åˆå§‹ä½ç‚¹ï¼ˆç¡®ä¿ total è¢« Hook æ•è·ï¼‰
            if self._ctx:
                self._ctx.report_progress(self._current_progress, self._total_progress)

    def close(self):
        if hasattr(self, '_reader') and self._reader: self._reader.close(); self._reader = None
        if hasattr(self, '_writer') and self._writer: self._writer.close(); self._writer = None
        if self._input_stream: self._input_stream.close()
        if self._output_stream: self._output_stream.close()
        
        old_status = self._status
        if self._shutdown_event.is_set(): self._status = NodeStatus.CANCELED
        elif self._status == NodeStatus.RUNNING: self._status = NodeStatus.COMPLETED
        self._end_time = time.time()

        # æ ¸å¿ƒä¿®å¤ï¼šå…³é—­å‰å¼ºåˆ¶åŒæ­¥ä¸€æ¬¡æœ€ç»ˆè¿›åº¦ï¼Œç¡®ä¿ total ä¸ä¸º 0
        if self._ctx:
            if self._total_progress < self._current_progress:
                self._total_progress = self._current_progress
            self._ctx.report_progress(self._current_progress, self._total_progress)
        
        _node_logger.info(f"{self.log_id} ğŸ èŠ‚ç‚¹å…³é—­ (Status: {old_status.value} -> {self._status.value}, Duration: {self.get_duration():.2f}s)")

    def cancel(self):
        if self._status == NodeStatus.RUNNING:
            self._status = NodeStatus.CANCELING
            self._shutdown_event.set()

    def _check_cancelled(self):
        if self._shutdown_event.is_set() or (self._ctx and self._ctx.is_cancelled()):
            raise InterruptedError()

    def get_duration(self) -> float:
        return (self._end_time or time.time()) - self._start_time if self._start_time > 0 else 0

    def get_progress(self) -> Any:
        return self._current_progress

    def _get_config_data(self) -> Dict[str, Any]:
        """è·å–èŠ‚ç‚¹çš„é…ç½®/èº«ä»½å‚æ•°ï¼ˆä¸å«è¿è¡ŒçŠ¶æ€ï¼‰"""
        return {
            "node_id": self._node_id,
            "input_uri": self._input_uri,
            "output_uri": self._output_uri,
            "protocol_prefix": self._protocol_prefix,
            "base_path": self._base_path
        }

    def get_runtime(self) -> Dict[str, Any]:
        """è·å–å®Œæ•´çš„è¿è¡Œæ—¶å¿«ç…§ï¼šé…ç½®å‚æ•° + ç‰©ç†çŠ¶æ€"""
        data = self._get_config_data()
        data.update({
            "status": self.status.value if hasattr(self.status, 'value') else self.status,
            "progress": self.progress,
            "duration": self.get_duration()
        })
        return data

    def resume_from_runtime(self, runtime_data: Dict[str, Any]) -> None:
        self._node_id = runtime_data.get("node_id", self._node_id)
        self._input_uri = runtime_data.get("input_uri", self._input_uri)
        self._output_uri = runtime_data.get("output_uri", self._output_uri)
        self._protocol_prefix = runtime_data.get("protocol_prefix", self._protocol_prefix)
        self._base_path = runtime_data.get("base_path", self._base_path)
        
        # æ ¸å¿ƒä¿®å¤ï¼šä¼˜å…ˆä»å¿«ç…§æ¢å¤çŠ¶æ€
        if "status" in runtime_data:
            st = runtime_data["status"]
            if isinstance(st, str):
                # å°è¯•ä»å­—ç¬¦ä¸²è½¬æ¢å› Enum
                for member in NodeStatus:
                    if member.value == st:
                        self._status = member
                        break
            elif isinstance(st, NodeStatus):
                self._status = st
        
        # å…œåº•ï¼šå¦‚æœæ²¡æ ‡è®°å®Œæˆï¼Œåˆ™æ ‡è®°ä¸ºæ¢å¤ä¸­
        if self._status != NodeStatus.COMPLETED:
            self._status = NodeStatus.RESUMING
            
        prog = runtime_data.get("progress", {})
        self._current_progress = prog.get("current", 0) if isinstance(prog, dict) else (prog or 0)
        self._total_progress = prog.get("total", 0) if isinstance(prog, dict) else 0
        
        # æ ¸å¿ƒä¿®å¤ï¼šå¿…é¡»è®°å½•æ¢å¤ä½ç‚¹ï¼Œä»¥ä¾¿ open() æ—¶ä¼ ç»™ Reader
        self._resume_progress = self._current_progress
        
        self._shutdown_event.clear()


# ==============================================================================
# 2. ç‹¬ç«‹å¼•æ“å®ç°ï¼šBatchNode & ParallelBatchNode
# ==============================================================================

class BatchNode(BaseNode, IBatchNode):
    """åŸå­å¼•æ“ï¼šé¡ºåºæ‰¹å¤„ç†"""
    def __init__(self, 
                 node_id: str = None, 
                 input_uri: Optional[str] = None,
                 output_uri: Optional[str] = None,
                 protocol_prefix: str = "",
                 base_path: str = "",
                 batch_size: int = 1):
        super().__init__(node_id=node_id, input_uri=input_uri, output_uri=output_uri, 
                         protocol_prefix=protocol_prefix, base_path=base_path)
        self._batch_size = batch_size
        self._processor: Optional[Callable[[List[Any], Any], List[Any]]] = None

    def set_processor(self, func: Callable[[List[Any], Any], List[Any]]):
        """è®¾ç½®ä¸šåŠ¡å¤„ç†å‡½æ•°"""
        self._processor = func

    def process_batch(self, data: List[Any]) -> List[Any]:
        if self._processor:
            return self._processor(data, self._ctx)
        return data

    def run(self):
        # æ ¸å¿ƒä¿®å¤ï¼šåªè¦ä¸æ˜¯å®Œæˆæˆ–å–æ¶ˆï¼Œéƒ½å¯ä»¥å°è¯•è¿è¡Œ (åŒ…å« RESUMING, FAILED)
        if self._status not in [NodeStatus.COMPLETED, NodeStatus.CANCELED]:
            if not getattr(self, '_reader', None):
                self.open(ctx=self._ctx)
        
        reader = getattr(self, '_reader', None)
        writer = getattr(self, '_writer', None)
        if not reader:
            _node_logger.warning(f"{self.log_id} âš ï¸ å¼•æ“æœªå°±ç»ª (æ—  Reader)ï¼Œè·³è¿‡æ‰§è¡Œ")
            return

        # æ ¸å¿ƒä¿®å¤ï¼šåŒæ­¥æ€»é‡ï¼ˆæµå¼æ¨¡å¼ä¸‹æ€»é‡ä¼šéšä¸Šæ¸¸å†™å…¥è€Œå¢é•¿ï¼‰
        self._total_progress = reader.total_count

        try:
            for data, ids in reader.read(batch_size=self._batch_size):
                self._check_cancelled()
                
                # At-most-once: è¯»åå³å­˜ã€‚ä¼˜ç‚¹ï¼šç»ä¸é‡å¤æ‰§è¡Œï¼ˆçœ Tokenï¼‰ï¼›ç¼ºç‚¹ï¼šå´©æºƒä¼šå¯¼è‡´æœ¬æ‰¹æ¬¡ä¸¢æ•°ã€‚
                self._current_progress = reader.completed_count
                if self._total_progress <= 0 or reader.total_count > self._total_progress:
                    self._total_progress = reader.total_count

                if self._ctx: 
                    self._ctx.report_progress(self._current_progress, self._total_progress)
                    if hasattr(self._ctx, "save_checkpoint"):
                        self._ctx.save_checkpoint()

                # ä¸°å¯Œæ—¥å¿—æ ¼å¼
                idx_range = f"{ids[0]}~{ids[-1]}" if len(ids) > 1 else f"{ids[0]}"
                _node_logger.info(f"{self.log_id} ğŸ“¥ è¯»å–æ‰¹æ¬¡: æ•°é‡={len(data)}, ç´¢å¼•={idx_range}")
                
                processed = self.process_batch(data)
                if writer and processed:
                    writer.write(processed, anchors=ids)
                    _node_logger.info(f"{self.log_id} ğŸ“¤ å†™å…¥å®Œæˆ: æˆåŠŸ={len(processed)}æ¡")
            
            if self._ctx: self._ctx.report_progress(self._total_progress, self._total_progress)
            
        except InterruptedError:
            self.cancel(); raise
        except Exception as e:
            _node_logger.error(f"{self.log_id} ğŸš¨ è¿è¡Œå´©æºƒ: {e}", exc_info=True)
            self._status = NodeStatus.FAILED; raise
        finally:
            self.close()

    def _get_config_data(self) -> Dict[str, Any]:
        data = super()._get_config_data()
        data.update({"batch_size": self._batch_size})
        return data

class ParallelBatchNode(BatchNode):
    """åŸå­å¼•æ“ï¼šå¹¶è¡Œæ‰¹å¤„ç†"""
    def __init__(self, 
                 node_id: str = None, 
                 input_uri: Optional[str] = None,
                 output_uri: Optional[str] = None,
                 protocol_prefix: str = "",
                 base_path: str = "",
                 batch_size: int = 1, 
                 parallel_size: int = 1):
        super().__init__(node_id=node_id, input_uri=input_uri, output_uri=output_uri, 
                         protocol_prefix=protocol_prefix, base_path=base_path, batch_size=batch_size)
        self._parallel_size = parallel_size

    def run(self):
        # æ ¸å¿ƒä¿®å¤ï¼šåªè¦ä¸æ˜¯å®Œæˆæˆ–å–æ¶ˆï¼Œéƒ½å¯ä»¥å°è¯•è¿è¡Œ (åŒ…å« RESUMING, FAILED)
        if self._status not in [NodeStatus.COMPLETED, NodeStatus.CANCELED]:
            if not getattr(self, '_reader', None):
                self.open(ctx=self._ctx)
        
        reader = getattr(self, '_reader', None)
        writer = getattr(self, '_writer', None)
        if not reader:
            _node_logger.warning(f"{self.log_id} âš ï¸ å¼•æ“æœªå°±ç»ª (æ—  Reader)ï¼Œè·³è¿‡æ‰§è¡Œ")
            return

        # æ ¸å¿ƒä¿®å¤ï¼šåªåŒæ­¥æ€»é‡
        self._total_progress = reader.total_count

        # æè‡´èƒŒå‹æ§åˆ¶
        semaphore = threading.BoundedSemaphore(self._parallel_size)
        futures = set()

        try:
            with ThreadPoolExecutor(max_workers=self._parallel_size) as executor:
                for data, ids in reader.read(batch_size=self._batch_size):
                    self._check_cancelled()
                    
                    # 1. é˜»å¡ç‚¹ï¼šå¦‚æœçº¿ç¨‹æ± å¤ªå¿™ï¼Œæ­¤å¤„ä¼šé˜»å¡
                    semaphore.acquire()
                    
                    # 2. At-most-once: æ´¾å‘å‰å³å­˜ç›˜ã€‚ç¡®ä¿ä»»åŠ¡ä¸€æ—¦è¿›å…¥é˜Ÿåˆ—å³è§†ä¸ºå·²å¤„ç†ï¼Œå³ä¾¿å­çº¿ç¨‹å´©æºƒä¹Ÿä¸é‡è·‘ã€‚
                    self._current_progress = reader.completed_count
                    if self._total_progress <= 0 or reader.total_count > self._total_progress:
                        self._total_progress = reader.total_count
                    
                    if self._ctx: 
                        self._ctx.report_progress(self._current_progress, self._total_progress)
                        if hasattr(self._ctx, "save_checkpoint"):
                            self._ctx.save_checkpoint()

                    # ä¸°å¯Œæ—¥å¿—æ ¼å¼
                    idx_range = f"{ids[0]}~{ids[-1]}" if len(ids) > 1 else f"{ids[0]}"
                    _node_logger.info(f"{self.log_id} ğŸ“¥ è¯»å–æ‰¹æ¬¡(å¹¶è¡Œ): æ•°é‡={len(data)}, ç´¢å¼•={idx_range}")
                    
                    def _safe_task(d, i):
                        try:
                            res = self.process_batch(d)
                            if writer and res:
                                writer.write(res, anchors=i)
                                _node_logger.info(f"{self.log_id} âœ… å¹¶è¡Œæ‰¹æ¬¡å®Œæˆ: {len(res)} æ¡")
                        except Exception as e:
                            _node_logger.error(f"{self.log_id} âŒ å¹¶è¡Œå¤„ç†å¤±è´¥: {e}", exc_info=True)
                            raise
                        finally:
                            # ä»»åŠ¡ç»“æŸï¼Œé‡Šæ”¾ä¿¡å·é‡
                            semaphore.release()

                    future = executor.submit(_safe_task, data, ids)
                    futures.add(future)
                    
                    # 5. åŠæ—¶æ¸…ç†å·²å®Œæˆçš„ future å¹¶æ£€æŸ¥å¼‚å¸¸
                    done_fs = {f for f in futures if f.done()}
                    for f in done_fs:
                        f.result() # å¦‚æœå­çº¿ç¨‹å´©æºƒï¼Œæ­¤å¤„ä¼šæŠ›å‡ºå¼‚å¸¸
                        futures.remove(f)
                
                # 6. ç­‰å¾…æ‰€æœ‰å‰©ä½™ä»»åŠ¡å®Œæˆ
                for f in futures:
                    f.result() 
            
            # 7. æœ€ç»ˆè¿›åº¦å¯¹é½
            if self._ctx: self._ctx.report_progress(self._total_progress, self._total_progress)
                
        except InterruptedError:
            self.cancel(); raise
        except Exception as e:
            self._status = NodeStatus.FAILED; raise
        finally:
            self.close()

    def _get_config_data(self) -> Dict[str, Any]:
        data = super()._get_config_data()
        data.update({"parallel_size": self._parallel_size})
        return data

# ==============================================================================
# 3. OperatorNode å®ç°ï¼šç»„åˆå®¹å™¨ (æ”¯æŒå†…éƒ¨å¹¶å‘é€‚é…)
# ==============================================================================

class OperatorNode(BatchNode):
    """é€šç”¨ç®—å­å®¹å™¨ï¼šè‡ªåŠ¨è¯†åˆ«å¹¶é€‚é… Batch/Single ç®—å­ï¼Œæ”¯æŒæ‰¹æ¬¡å†…å¹¶å‘"""
    def __init__(self, 
                 node_id: str, 
                 operator: IOperator, 
                 input_uri: Optional[str] = None,
                 output_uri: Optional[str] = None,
                 protocol_prefix: str = "",
                 base_path: str = "",
                 batch_size: int = 1):
        super().__init__(node_id=node_id, input_uri=input_uri, output_uri=output_uri, 
                         protocol_prefix=protocol_prefix, base_path=base_path, batch_size=batch_size)
        self._operator = operator

    def process_batch(self, data: List[Any]) -> List[Any]:
        # 1. åœºæ™¯ Aï¼šç®—å­åŸç”Ÿæ”¯æŒæ‰¹é‡ (IBatchOperator) -> ç›´æ¥äº¤ç»™ç®—å­æ‰§è¡Œ
        if hasattr(self._operator, "process_batch"):
            return self._operator.process_batch(data, ctx=self._ctx)
        
        # 2. åœºæ™¯ Bï¼šç®—å­åªæœ‰å•æ¡å¤„ç†èƒ½åŠ› (ISingleOperator) -> å®¹å™¨å±‚æ ¹æ® batch_size æ‰§è¡Œæ‰¹æ¬¡å†…å¹¶å‘
        if hasattr(self._operator, "process_item"):
            current_batch_size = len(data)
            
            # å¦‚æœæ‰¹æ¬¡å¤§äº 1ï¼Œåˆ™å¼€å¯æ‰¹æ¬¡å†…å¹¶å‘ä»¥å‹æ¦¨æ€§èƒ½
            if current_batch_size > 1:
                with ThreadPoolExecutor(max_workers=current_batch_size) as executor:
                    futures = [executor.submit(self._operator.process_item, item, self._ctx) for item in data]
                    results = []
                    for f in futures:
                        res = f.result()
                        # å…¼å®¹ 1:N çˆ†ç‚¸åˆ†å‘ä¸è¿‡æ»¤
                        if isinstance(res, list): results.extend(res)
                        elif res is not None: results.append(res)
                    return results
            else:
                # åªæœ‰ 1 æ¡æ•°æ®æ—¶ï¼Œä¿æŒä¸²è¡Œè°ƒç”¨
                res = self._operator.process_item(data[0], ctx=self._ctx)
                if isinstance(res, list): return res
                return [res] if res is not None else []
        
        raise TypeError(f"ç®—å­ {self._operator.__class__.__name__} æœªå®ç° process_batch æˆ– process_item")

class ParallelOperatorNode(ParallelBatchNode):
    """å¹¶è¡Œç®—å­å®¹å™¨ï¼šæ‰‹åŠ¨å®ç°è‡ªé€‚åº” process_batch ä»¥è§„é¿ MRO å†²çª"""
    def __init__(self, 
                 node_id: str, 
                 operator: IOperator, 
                 input_uri: Optional[str] = None,
                 output_uri: Optional[str] = None,
                 protocol_prefix: str = "",
                 base_path: str = "",
                 batch_size: int = 1, 
                 parallel_size: int = 1):
        super().__init__(node_id=node_id, input_uri=input_uri, output_uri=output_uri, 
                         protocol_prefix=protocol_prefix, base_path=base_path, 
                         batch_size=batch_size, parallel_size=parallel_size)
        self._operator = operator

    def process_batch(self, data: List[Any]) -> List[Any]:
        # 1. åœºæ™¯ Aï¼šç®—å­åŸç”Ÿæ”¯æŒæ‰¹é‡ (IBatchOperator) -> ç›´æ¥äº¤ç»™ç®—å­æ‰§è¡Œ
        if hasattr(self._operator, "process_batch"):
            return self._operator.process_batch(data, ctx=self._ctx)
        
        # 2. åœºæ™¯ Bï¼šç®—å­åªæœ‰å•æ¡å¤„ç†èƒ½åŠ› (ISingleOperator) -> å®¹å™¨å±‚æ ¹æ® batch_size æ‰§è¡Œæ‰¹æ¬¡å†…å¹¶å‘
        if hasattr(self._operator, "process_item"):
            current_batch_size = len(data)
            
            # å¦‚æœæ‰¹æ¬¡å¤§äº 1ï¼Œåˆ™å¼€å¯æ‰¹æ¬¡å†…å¹¶å‘ä»¥å‹æ¦¨æ€§èƒ½
            if current_batch_size > 1:
                with ThreadPoolExecutor(max_workers=current_batch_size) as executor:
                    futures = [executor.submit(self._operator.process_item, item, self._ctx) for item in data]
                    results = []
                    for f in futures:
                        res = f.result()
                        # å…¼å®¹ 1:N çˆ†ç‚¸åˆ†å‘ä¸è¿‡æ»¤
                        if isinstance(res, list): results.extend(res)
                        elif res is not None: results.append(res)
                    return results
            else:
                # åªæœ‰ 1 æ¡æ•°æ®æ—¶ï¼Œä¿æŒä¸²è¡Œè°ƒç”¨
                res = self._operator.process_item(data[0], ctx=self._ctx)
                if isinstance(res, list): return res
                return [res] if res is not None else []
        
        raise TypeError(f"ç®—å­ {self._operator.__class__.__name__} æœªå®ç° process_batch æˆ– process_item")

# å‘åå…¼å®¹åˆ«åï¼šç°åœ¨æ‰€æœ‰ç®—å­èŠ‚ç‚¹éƒ½å…·å¤‡è‡ªé€‚åº”èƒ½åŠ›
BatchOperatorNode = OperatorNode
SingleOperatorNode = OperatorNode
ParallelBatchOperatorNode = ParallelOperatorNode
ParallelSingleOperatorNode = ParallelOperatorNode


# ==============================================================================
# 4. ç»Ÿä¸€ä»£ç†ï¼šä½œä¸ºè“å›¾å‚æ•°çš„ç»ˆç‚¹æŒæœ‰è€…
# ==============================================================================

class UnifiedNode(BaseNode):
    """ä»£ç†èŠ‚ç‚¹ï¼šæŒæœ‰æ‰€æœ‰æ„å›¾å‚æ•°å¹¶åœ¨è¿è¡Œæ—¶å®ä¾‹åŒ–å¼•æ“"""
    def __init__(self, 
                 node_id: str = None,
                 input_uri: Optional[str] = None,
                 output_uri: Optional[str] = None,
                 protocol_prefix: str = "",
                 base_path: str = "",
                 batch_size: int = 1, 
                 parallel_size: int = 1):
        super().__init__(node_id=node_id, input_uri=input_uri, output_uri=output_uri, 
                         protocol_prefix=protocol_prefix, base_path=base_path)
        self._batch_size = batch_size
        self._parallel_size = parallel_size
        self._processor = None
        self._impl: Optional[BatchNode] = None

    def set_processor(self, func):
        """è®¾ç½®ä¸šåŠ¡å¤„ç†å‡½æ•°ï¼Œå¹¶åŒæ­¥ç»™å†…éƒ¨å¼•æ“"""
        self._processor = func
        if self._impl:
            self._impl.set_processor(func)

    def _ensure_impl(self):
        if self._impl: return
        
        # æ ¸å¿ƒï¼šå®Œå…¨æ˜¾å¼åˆå§‹åŒ–å¼•æ“
        if self._parallel_size > 1:
            self._impl = ParallelBatchNode(
                node_id=self._node_id,
                input_uri=self._input_uri,
                output_uri=self._output_uri,
                protocol_prefix=self._protocol_prefix,
                base_path=self._base_path,
                batch_size=self._batch_size,
                parallel_size=self._parallel_size
            )
        else:
            self._impl = BatchNode(
                node_id=self._node_id,
                input_uri=self._input_uri,
                output_uri=self._output_uri,
                protocol_prefix=self._protocol_prefix,
                base_path=self._base_path,
                batch_size=self._batch_size
            )
            
        # åŒæ­¥å¤„ç†å™¨
        if self._processor:
            self._impl.set_processor(self._processor)
            
        # æ ¸å¿ƒä¿®å¤ï¼šå°†é—¨é¢æš‚å­˜çš„â€œçœŸç›¸ä½ç‚¹â€åŒæ­¥ç»™å†…éƒ¨å¼•æ“
        facade_rt = {
            "node_id": self._node_id,
            "status": self._status,
            "progress": {"current": self._current_progress, "total": self._total_progress},
            "batch_size": self._batch_size,
            "parallel_size": self._parallel_size,
            "input_uri": self._input_uri,
            "output_uri": self._output_uri,
            "protocol_prefix": self._protocol_prefix,
            "base_path": self._base_path
        }
        self._impl.resume_from_runtime(facade_rt)

        if self._ctx: self._impl.set_context(self._ctx)
        if self._input_stream: self._impl.bind_io(self._input_stream, self._output_stream)

    @property
    def status(self) -> NodeStatus: return self._impl.status if self._impl else self._status
    
    @status.setter
    def status(self, value: NodeStatus):
        self._status = value
        if self._impl: self._impl.status = value

    @property
    def progress(self) -> Dict[str, int]: return self._impl.progress if self._impl else super().progress

    def bind_io(self, in_s, out_s):
        super().bind_io(in_s, out_s)
        if self._impl: self._impl.bind_io(in_s, out_s)
    
    def set_context(self, ctx: NodeContextImpl):
        super().set_context(ctx)
        if self._impl: self._impl.set_context(ctx)

    def open(self, ctx=None, progress=None):
        if ctx: self.set_context(ctx)
        self._ensure_impl()
        # å½»åº•å§”æ‰˜ç»™å†…éƒ¨å¼•æ“ï¼ŒåŒæ­¥ä½ç‚¹
        self._impl.open(ctx=self._ctx, progress=progress)
        self._status = self._impl.status
        self._current_progress = self._impl._current_progress
        self._total_progress = self._impl._total_progress

    def run(self): 
        self._ensure_impl()
        # æ ¸å¿ƒä¿®å¤ï¼šå¢åŠ å¹‚ç­‰ä¿æŠ¤ï¼Œåªæœ‰åœ¨æœªå‡†å¤‡å¥½ Reader æ—¶æ‰è°ƒç”¨ open
        # è¿™é˜²æ­¢äº† StreamingPipeline çš„â€œé¢„çƒ­é˜¶æ®µâ€ä¸â€œæ‰§è¡Œé˜¶æ®µâ€é‡å¤æ‰“å¼€ IO
        if self._status not in [NodeStatus.COMPLETED, NodeStatus.CANCELED]:
            if self._impl and not getattr(self._impl, '_reader', None):
                self.open(ctx=self._ctx)
            
        # å†…éƒ¨å¼•æ“æ‰§è¡Œ
        self._impl.run()
        # åŒæ­¥çŠ¶æ€å’Œè¿›åº¦ç»™é—¨é¢
        self._status = self._impl.status
        self._current_progress = self._impl._current_progress
        self._total_progress = self._impl._total_progress

    def close(self): 
        if self._impl: 
            self._impl.close()
            self._status = self._impl.status
        super().close()
    def cancel(self): 
        if self._impl: self._impl.cancel()
    def get_duration(self) -> float: 
        return self._impl.get_duration() if self._impl else super().get_duration()
    def get_progress(self) -> Any: 
        return self._impl.get_progress() if self._impl else self._current_progress
    def get_runtime(self) -> Dict[str, Any]: 
        rt = self._impl.get_runtime() if self._impl else super().get_runtime()
        rt.update({"batch_size": self._batch_size, "parallel_size": self._parallel_size})
        return rt

    def resume_from_runtime(self, runtime_data: Dict[str, Any]) -> None: 
        super().resume_from_runtime(runtime_data)
        self._batch_size = runtime_data.get("batch_size", self._batch_size)
        self._parallel_size = runtime_data.get("parallel_size", self._parallel_size)
        if self._impl: self._impl.resume_from_runtime(runtime_data)

class UnifiedOperatorNode(UnifiedNode):
    def __init__(self, 
                 operator: IOperator,
                 node_id: str = None,
                 input_uri: Optional[str] = None,
                 output_uri: Optional[str] = None,
                 protocol_prefix: str = "",
                 base_path: str = "",
                 batch_size: int = 1, 
                 parallel_size: int = 1):
        super().__init__(node_id=node_id, input_uri=input_uri, output_uri=output_uri, 
                         protocol_prefix=protocol_prefix, base_path=base_path, 
                         batch_size=batch_size, parallel_size=parallel_size)
        self._operator = operator

    def _ensure_impl(self):
        if self._impl: return
        
        # æ ¸å¿ƒï¼šæ ¹æ®å¹¶å‘è§„æ¨¡é€‰æ‹©å®¹å™¨ï¼Œç®—å­å†…éƒ¨çš„å¤šæ€ç”± OperatorNode è‡ªè¡Œå¤„ç†
        if self._parallel_size > 1:
            self._impl = ParallelOperatorNode(
                node_id=self._node_id,
                operator=self._operator,
                input_uri=self._input_uri,
                output_uri=self._output_uri,
                protocol_prefix=self._protocol_prefix,
                base_path=self._base_path,
                batch_size=self._batch_size,
                parallel_size=self._parallel_size
            )
        else:
            self._impl = OperatorNode(
                node_id=self._node_id,
                operator=self._operator,
                input_uri=self._input_uri,
                output_uri=self._output_uri,
                protocol_prefix=self._protocol_prefix,
                base_path=self._base_path,
                batch_size=self._batch_size
            )
        
        # æ ¸å¿ƒä¿®å¤ï¼šå°†é—¨é¢æš‚å­˜çš„â€œçœŸç›¸ä½ç‚¹â€åŒæ­¥ç»™å†…éƒ¨å¼•æ“
        facade_rt = {
            "node_id": self._node_id,
            "status": self._status,
            "progress": {"current": self._current_progress, "total": self._total_progress},
            "batch_size": self._batch_size,
            "parallel_size": self._parallel_size,
            "input_uri": self._input_uri,
            "output_uri": self._output_uri,
            "protocol_prefix": self._protocol_prefix,
            "base_path": self._base_path
        }
        self._impl.resume_from_runtime(facade_rt)

        if self._ctx: self._impl.set_context(self._ctx)
        if self._input_stream: self._impl.bind_io(self._input_stream, self._output_stream)

    def get_runtime(self) -> Dict[str, Any]:
        rt = super().get_runtime()
        rt["operator_type"] = self._operator.__class__.__name__ if self._operator else None
        return rt

class InputNode(UnifiedNode):
    def __init__(self, 
                 input_uri: Optional[str] = None,
                 output_uri: Optional[str] = None,
                 protocol_prefix: str = "",
                 base_path: str = "",
                 batch_size: int = 1, 
                 parallel_size: int = 1):
        super().__init__(node_id="input", input_uri=input_uri, output_uri=output_uri, 
                         protocol_prefix=protocol_prefix, base_path=base_path, 
                         batch_size=batch_size, parallel_size=parallel_size)

    def open(self, ctx=None, progress=None):
        super().open(ctx, progress)
        # æ ¸å¿ƒä¿®å¤ï¼šInputNode çš„è¾“å…¥æºå¿…ç„¶æ˜¯é™æ€çš„ï¼ˆå¦‚åˆå§‹ JSONL æ–‡ä»¶ï¼‰
        # æˆ‘ä»¬å¿…é¡»ç»™å®ƒçš„ Reader Channel è´´ä¸Š EOF æ ‡ç­¾ï¼Œå¦åˆ™ StreamBridge ä¼šæ­»ç­‰å°æ¡
        if self._impl and hasattr(self._impl, "_reader") and self._impl._reader:
            if hasattr(self._impl._reader, "channel"):
                self._impl._reader.channel.set_eof()
                _node_logger.info(f"{self.log_id} å·²å°†è¾“å…¥æºæ ‡è®°ä¸ºé™æ€ (EOF)")

class OutputNode(UnifiedNode):
    def __init__(self, 
                 input_uri: Optional[str] = None,
                 output_uri: Optional[str] = None,
                 protocol_prefix: str = "",
                 base_path: str = "",
                 batch_size: int = 1, 
                 parallel_size: int = 1):
        super().__init__(node_id="output", input_uri=input_uri, output_uri=output_uri, 
                         protocol_prefix=protocol_prefix, base_path=base_path, 
                         batch_size=batch_size, parallel_size=parallel_size)
