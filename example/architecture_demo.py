"""
DataGen æ¶æ„æ·±åº¦æµ‹è¯•é›† (Architecture Deep Test Suite)
1. æç®€æ¨¡å¼ (Case 1: Path Derivation)
2. æµå¼æ¨¡å¼ (Case 2: Streaming)
3. å®¹é”™æ¢å¤ (Case 3: Recovery & Mirror)
4. å¤šçº§ç®—å­ä¸è¿‡æ»¤ (Case 4: Long Chain & Filter) - æµ‹è¯•æ•°æ®é‡å˜åŒ–æ—¶çš„è¿›åº¦å‡†ç¡®æ€§
5. æ‰‹åŠ¨è·¯å¾„è¦†ç›– (Case 5: Manual URI Override) - æµ‹è¯•è‡ªå®šä¹‰ä¸­é—´è·¯å¾„
6. é«˜å¹¶å‘èƒŒå‹æµ‹è¯• (Case 6: Backpressure Stress) - æµ‹è¯• Semaphore ç¨³å®šæ€§
"""
import time
import os
import json
import threading
from typing import List, Any, Optional, Dict

# ç»Ÿä¸€ä» datagen é¡¶å±‚å¯¼å‡º
from llm_datagen import (
    UnifiedNodePipeline,
    UnifiedPipeline,
    GenericLLMOperator,
    FunctionOperator,
    UnifiedNode,
    WriterConfig
)

# ========== ç¬¬ä¸€æ­¥ï¼šæ³¨å†Œ LLM æ¨¡å‹ ==========
def setup_llm_model():
    """è®¾ç½® LLM æ¨¡å‹"""
    from llm_datagen.llm import model_container
    try:
        # ä»åŒç›®å½•ä¸‹çš„ hello.txt è·å– api_key
        key_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "hello.txt")
        api_key = ""
        if os.path.exists(key_path):
            with open(key_path, "r", encoding="utf-8") as f:
                api_key = f.read().strip()
        
        model_container.register(
            name="example_model",
            model="doubao-seed-1-6-251015",
            base_url="https://ark.cn-beijing.volces.com/api/v3",
            api_key=api_key,
            default_params={"temperature": 0.7, "max_tokens": 1000}
        )
        print("âœ“ LLM æ¨¡å‹æ³¨å†ŒæˆåŠŸ (Doubao)")
    except Exception as e:
        print(f"âš ï¸ LLM æ³¨å†Œå¼‚å¸¸: {e} (ç³»ç»Ÿå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼è¿è¡Œ)")

# ========== ç¬¬äºŒæ­¥ï¼šå®šä¹‰ä¸šåŠ¡ç®—å­ ==========

class TranslationOperator(GenericLLMOperator):
    """ç¿»è¯‘ç®—å­ï¼šåœ¨æ— ç½‘ç»œç¯å¢ƒä¸‹è‡ªåŠ¨åˆ‡æ¢ä¸ºæ¨¡æ‹Ÿé€»è¾‘"""
    def __init__(self, model_name: str = "example_model"):
        super().__init__(config={
            "model_name": model_name,
            "custom_prompt": "è¯·å°†ä»¥ä¸‹æ–‡å­—ç¿»è¯‘æˆè‹±æ–‡ï¼š{text}"
        })

    def process_batch(self, items: List[Any], ctx: Any = None) -> List[Any]:
        try:
            # æ¨¡æ‹Ÿåœ¨æ²™ç®±ç¯å¢ƒä¸­å¯èƒ½å‘ç”Ÿçš„ç½‘ç»œè¿æ¥å¤±è´¥æˆ–ç©ºè¿”å›
            res = super().process_batch(items, ctx)
            # print(f"TranslationOperator1234: {res}")
            if not res or all(not r.get("llm_output") for r in res): raise RuntimeError()
            return res
        except:
            results = []
            mock_translations = ["The weather is nice.", "AI is changing life.", "Code changes world."]
            for i, item in enumerate(items):
                text = item.get("text", "") if isinstance(item, dict) else str(item)
                res_item = item.copy() if isinstance(item, dict) else {"text": text}
                res_item["llm_output"] = f"[SIM] {mock_translations[i % len(mock_translations)]}"
                if ctx: ctx.report_usage({"prompt_tokens": len(text), "completion_tokens": 20})
                results.append(res_item)
            return results

class FilterOperator(FunctionOperator):
    """è¿‡æ»¤ç®—å­ï¼šä»…ä¿ç•™é•¿åº¦å¤§äº 15 çš„ç»“æœ"""
    def __init__(self, min_len: int = 15):
        def filter_func(item: Any):
            text = item.get("llm_output", "")
            # è¿”å›ç©ºåˆ—è¡¨è¡¨ç¤ºè¿‡æ»¤æ‰è¯¥é¡¹
            return [item] if len(text) > min_len else []
        super().__init__(func=filter_func)

class WordCountOperator(FunctionOperator):
    """ç»Ÿè®¡ç®—å­ï¼šç»Ÿè®¡å•è¯ä¸ªæ•°"""
    def __init__(self):
        def count_words(item: Any):
            text = item.get("llm_output", "")
            item["word_count"] = len([w for w in text.split() if w.strip()])
            return item
        super().__init__(func=count_words)

class SegmentOperator(FunctionOperator):
    """åˆ†è¯ç®—å­ï¼šå°†ä¸€æ®µæ–‡å­—æ‹†åˆ†ä¸ºå¤šä¸ªå•è¯é¡¹ (1:N æ¨¡å¼)"""
    def __init__(self):
        def segment_func(item: Any):
            text = item.get("llm_output", "")
            # ç®€å•æ¨¡æ‹Ÿåˆ†è¯ï¼šæŒ‰ç©ºæ ¼æˆ–æ ‡ç‚¹æ‹†åˆ†
            words = text.replace(".", "").replace(",", "").split()
            # æ ¸å¿ƒæ”¹è¿›ï¼šä½¿ç”¨æ¡†æ¶è‡ªåŠ¨æ³¨å…¥çš„ _i ä½œä¸ºç‰©ç†æº¯æº ID
            parent_i = item.get("_i")
            # è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¡†æ¶ä¼šè‡ªåŠ¨å°†å…¶æ‰å¹³åŒ–å¹¶ä½œä¸ºå¤šä¸ªç‹¬ç«‹ items è¾“å‡º
            return [{"word": w, "parent_i": parent_i} for w in words]
        super().__init__(func=segment_func)

def setup_data(file_path, count=100):
    """å‡†å¤‡æ¨¡æ‹Ÿæ•°æ®"""
    os.makedirs(os.path.dirname(os.path.abspath(file_path)), exist_ok=True)
    sample_texts = ["ä»Šå¤©å¤©æ°”çœŸå¥½", "äººå·¥æ™ºèƒ½æ”¹å˜ç”Ÿæ´»", "ä»£ç æ”¹å˜ä¸–ç•Œ"]
    with open(file_path, 'w', encoding='utf-8') as f:
        for i in range(count):
            text = sample_texts[i % len(sample_texts)]
            f.write(json.dumps({"text": text, "id": i}, ensure_ascii=False) + '\n')

# ========== åœºæ™¯ 1-3 (ä¿æŒåŸæœ‰é€»è¾‘) ==========
def run_case_1():
    print("\n>>> åœºæ™¯ 1: æç®€æ¨¡å¼ (è‡ªåŠ¨è·¯å¾„æ¨å¯¼)")
    input_file = "tmp/demo/case1_input.jsonl"
    setup_data(input_file, count=20)
    
    # æ–°æ¨èå†™æ³•ï¼šå‚æ•°æ”¶æ•›åˆ°æ„é€ å‡½æ•°
    pipeline = UnifiedPipeline(
        operators=[TranslationOperator(), WordCountOperator()],
        input_uri=f"jsonl://{input_file}", 
        output_uri="jsonl://tmp/demo/result1.jsonl", 
        base_path="tmp/",
        protocol_prefix="hello"
    )
    pipeline.create(pipeline_id="case1_simple")
    pipeline.run()

def run_case_2():
    print("\n>>> åœºæ™¯ 2: å¹¶è¡Œæµå¼æ¨¡å¼ (Streaming Engine)")
    input_file = "tmp/demo/case2_input.jsonl"
    setup_data(input_file, count=50)
    
    # æ”¯æŒåœ¨æ„é€ å‡½æ•°ä¸­æŒ‡å®šæ‰§è¡Œæ¨¡å¼
    pipeline = UnifiedPipeline(
        operators=[TranslationOperator(), WordCountOperator()],
        input_uri=f"jsonl://{input_file}", 
        output_uri="jsonl://tmp/demo/case2_out.jsonl", 
        streaming=True, 
        parallel_size=5
    )
    pipeline.create(pipeline_id="case2_streaming")
    pipeline.run()

def run_case_3():
    writer_cfg = WriterConfig(
        async_mode=True,
        flush_batch_size=50,
        flush_interval=20,
        queue_size=1000
    )
    print("\n>>> åœºæ™¯ 3: è‡ªåŠ¨æ–­ç‚¹æ¢å¤æ¨¡å¼ (Mirror Recovery)")
    pid = "case3_recovery"
    input_file = "tmp/demo/case3_input.jsonl"
    setup_data(input_file, count=40)
    
    # [A] æ¨¡æ‹Ÿå´©æºƒè¿è¡Œ
    pipe1 = UnifiedPipeline(
        operators=[TranslationOperator()],
        input_uri=f"jsonl://{input_file}", 
        output_uri="jsonl://tmp/demo/case3_fail.jsonl",
        batch_size=2,

        writer_config=writer_cfg
    )
    pipe1.create(pipeline_id=pid)
    
    # æ³¨æ„ï¼šnodes[0] æ˜¯ InputNode, nodes[1] æ˜¯ç¬¬ä¸€ä¸ªç®—å­èŠ‚ç‚¹
    target_node = pipe1.nodes[1]
    
    # æ¨¡æ‹Ÿå´©æºƒé€»è¾‘
    target_node._ensure_impl()
    original_p = target_node._impl.process_batch
    def crash_p(*args, **kwargs):
        if target_node.get_progress() >= 15: raise Exception("Crash simulated")
        return original_p(*args, **kwargs)
    target_node._impl.process_batch = crash_p
    
    try: pipe1.run()
    except: print(f"âœ“ æ¨¡æ‹Ÿå´©æºƒå®Œæˆï¼Œå½“å‰è¿›åº¦: {target_node.get_progress()}")

    # [B] æ¢å¤è¿è¡Œ
    print("\n--- [B] æ­£åœ¨æ‰§è¡Œè‡ªåŠ¨æ¢å¤ ---")
    pipe2 = UnifiedPipeline(operators=[TranslationOperator()])
    pipe2.resume(pipeline_id=pid)
    pipe2.run()

# ========== æ–°åœºæ™¯ 4: å¤šçº§ç®—å­é•¿é“¾ä¸è¿‡æ»¤ ==========
def run_case_4():
    print("\n>>> åœºæ™¯ 4: å¤šçº§é•¿é“¾ + è¿‡æ»¤ (æµ‹è¯•é 1:1 æ•°æ®æµ)")
    input_file = "tmp/demo/case4_input.jsonl"
    setup_data(input_file, count=50)
    writer_cfg = WriterConfig(
        async_mode=True,
        flush_batch_size=50,
        flush_interval=20,
        queue_size=1000
    )
    pipeline = UnifiedPipeline(
        operators=[
            TranslationOperator(), 
            FilterOperator(min_len=50),
            WordCountOperator()
        ],
        input_uri=f"jsonl://{input_file}",
        output_uri="jsonl://tmp/demo/case4_final.jsonl",
        batch_size=5,
        writer_config=writer_cfg
    )
    pipeline.create(pipeline_id="case4_filter_chain")
    pipeline.run()
    print("âœ“ åœºæ™¯ 4 å®Œæˆã€‚")

# ========== æ–°åœºæ™¯ 5: æ‰‹åŠ¨ URI è·¯å¾„è¦†ç›– ==========
def run_case_5():
    print("\n>>> åœºæ™¯ 5: æ‰‹åŠ¨ URI è·¯å¾„è¦†ç›– (æµ‹è¯•æ˜¾å¼è·¯å¾„ä¼˜å…ˆçº§)")
    input_file = "tmp/demo/case5_input.jsonl"
    setup_data(input_file, count=10)
    
    pipeline = UnifiedPipeline(
        operators=[TranslationOperator(), WordCountOperator()],
        input_uri=f"jsonl://{input_file}",
        output_uri="jsonl://tmp/demo/case5_final.jsonl"
    )
    
    # é€šè¿‡ node_configs æ‰‹åŠ¨æŒ‡å®š node_0 çš„è¾“å‡ºè·¯å¾„
    custom_uri = "jsonl://tmp/custom_location/intermediate_data.jsonl"
    
    pipeline.create(
        pipeline_id="case5_override",
        node_configs=[
            {"output_uri": custom_uri}, 
            {}
        ]
    )
    
    print(f"--- éªŒè¯è·¯å¾„è¦†ç›– ---")
    rt = pipeline.get_runtime()
    n0_out = rt['nodes'][1]['output_uri']
    n1_in = rt['nodes'][2]['input_uri']
    print(f"Node 0 Out: {n0_out}")
    print(f"Node 1 In : {n1_in}")
    
    if n0_out == custom_uri and n1_in == custom_uri:
        print("âœ“ è·¯å¾„è¦†ç›–ä¸è‡ªåŠ¨ç„Šæ¥åŒæ­¥æ ¡éªŒæˆåŠŸ")
    else:
        print("âŒ è·¯å¾„è¦†ç›–å¤±æ•ˆ")
    pipeline.run()

# ========== æ–°åœºæ™¯ 6: é«˜å¹¶å‘èƒŒå‹å‹åŠ›æµ‹è¯• ==========
def run_case_6():
    print("\n>>> åœºæ™¯ 6: é«˜å¹¶å‘èƒŒå‹æµ‹è¯• (1000æ¡æ•°æ® / 20å¹¶å‘)")
    input_file = "tmp/demo/case6_input.jsonl"
    setup_data(input_file, count=1000)
    
    pipeline = UnifiedPipeline(
        operators=[TranslationOperator()],
        input_uri=f"jsonl://{input_file}",
        output_uri="jsonl://tmp/demo/case6_final.jsonl",
        parallel_size=20,
        batch_size=5
    )
    pipeline.create(pipeline_id="case6_stress")
    
    print("ğŸš€ å¯åŠ¨é«˜å‹åŠ›æµ‹è¯•...")
    start = time.time()
    pipeline.run()
    print(f"âœ“ åœºæ™¯ 6 å®Œæˆï¼Œæ€»è€—æ—¶: {time.time()-start:.2f}s")

# ========== æ–°åœºæ™¯ 7: çˆ†ç‚¸åˆ†å‘æ¨¡å¼ (1:N æ¨¡å¼) + ä¸¤æ¬¡æ¢å¤æµ‹è¯• ==========
def run_case_7():
    print("\n>>> åœºæ™¯ 7: çˆ†ç‚¸åˆ†å‘ (1:N æ¨¡å¼) + ä¸¤æ¬¡æ¢å¤æµ‹è¯•")
    pid = "case7_explosion"
    input_file = "tmp/demo/case7_input.jsonl"
    setup_data(input_file, count=50)
    
    def inject_crash(node, threshold, message="Crash simulated"):
        node._ensure_impl()
        original_p = node._impl.process_batch
        def crash_p(*args, **kwargs):
            if node.get_progress() >= threshold:
                raise Exception(f"{message} at {threshold}")
            return original_p(*args, **kwargs)
        node._impl.process_batch = crash_p

    print("\n--- [A] ç¬¬ä¸€æ¬¡è¿è¡Œï¼šåœ¨ Translation (node_0) é˜¶æ®µæ¨¡æ‹Ÿå´©æºƒ ---")
    pipe1 = UnifiedPipeline(
        operators=[TranslationOperator(), SegmentOperator()],
        input_uri=f"jsonl://{input_file}",
        output_uri="jsonl://tmp/demo/case7_final.jsonl",
        streaming=True,
        batch_size=5
    )
    pipe1.create(pipeline_id=pid)
    inject_crash(pipe1.nodes[1], 20, "Translation Crash")
    
    try: pipe1.run()
    except Exception as e: print(f"âœ“ æ•è·åˆ°é¢„æœŸå´©æºƒ: {e}")

    print("\n--- [B] ç¬¬äºŒæ¬¡è¿è¡Œï¼šæ¢å¤ï¼Œå¹¶åœ¨ Segmentation (node_1) é˜¶æ®µæ¨¡æ‹Ÿç¬¬äºŒæ¬¡å´©æºƒ ---")
    pipe2 = UnifiedPipeline(operators=[TranslationOperator(), SegmentOperator()])
    pipe2.resume(pipeline_id=pid)
    inject_crash(pipe2.nodes[2], 35, "Segmentation Crash")
    
    try: pipe2.run()
    except Exception as e: print(f"âœ“ æ•è·åˆ°ç¬¬äºŒæ¬¡é¢„æœŸå´©æºƒ: {e}")

    print("\n--- [C] ç¬¬ä¸‰æ¬¡è¿è¡Œï¼šæœ€ç»ˆæ¢å¤å¹¶å®Œæˆ ---")
    pipe3 = UnifiedPipeline(operators=[TranslationOperator(), SegmentOperator()])
    pipe3.resume(pipeline_id=pid)
    pipe3.run()
    
    # éªŒè¯æœ€ç»ˆç»“æœ
    output_path = "tmp/demo/case7_final.jsonl"
    if os.path.exists(output_path):
        with open(output_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            print(f"âœ“ åœºæ™¯ 7 æœ€ç»ˆå®Œæˆã€‚50 æ¡è¾“å…¥äº§ç”Ÿäº† {len(lines)} æ¡åˆ†è¯è¾“å‡ºã€‚")

def run_case_8():
    """æ–°å¢åœºæ™¯ï¼šUnifiedNodePipeline ç›´æ¥ä½¿ç”¨ç‰©ç†èŠ‚ç‚¹"""
    print("\n>>> åœºæ™¯ 8: ç‰©ç† Master æ¨¡å¼ (ç›´æ¥æ“ä½œ UnifiedNode)")
    input_file = "tmp/demo/case8_input.jsonl"
    setup_data(input_file, count=10)
    
    from llm_datagen import StreamFactory
    
    # 1. æ‰‹åŠ¨æ„å»ºç‰©ç†èŠ‚ç‚¹
    node1 = UnifiedNode(node_id="manual_n1", batch_size=2)
    
    # æ‰‹åŠ¨åˆ›å»ºçš„è¾“å…¥æµï¼Œå¦‚æœæ˜¯é™æ€æ–‡ä»¶ï¼Œéœ€è¦ç«‹å³å°å£(seal)ï¼Œå¦åˆ™ Reader ä¼šä¸€ç›´ç­‰å¾…
    in_s = StreamFactory.create(f"jsonl://{input_file}")
    in_s.seal() 
    
    node1.bind_io(in_s, StreamFactory.create("jsonl://tmp/demo/case8_mid.jsonl"))
    # æ³¨å…¥ç®€å•é€»è¾‘
    node1.set_processor(lambda items, ctx: [{"text": item.get("text", "") + " [Processed]"} for item in items])
    
    # 2. ä½¿ç”¨ UnifiedNodePipeline (ç‰©ç† Master)
    pipeline = UnifiedNodePipeline(nodes=[node1])
    pipeline.create(pipeline_id="case8_physical", streaming=False)
    pipeline.run()
    print("âœ“ åœºæ™¯ 8 å®Œæˆã€‚")

# ========== æ–°åœºæ™¯ 9: å¼‚æ­¥å†™å…¥å‹åŠ›æµ‹è¯• (WriterConfig Stress Test) ==========
def run_case_9():
    print("\n>>> åœºæ™¯ 9: å¼‚æ­¥å†™å…¥å‹åŠ›æµ‹è¯• (WriterConfig Stress Test)")
    input_file = "tmp/demo/case9_input.jsonl"
    setup_data(input_file, count=10000)
    
    # é…ç½®é«˜æ€§èƒ½å¼‚æ­¥å†™å…¥ç­–ç•¥
    writer_cfg = WriterConfig(
        async_mode=True,
        flush_batch_size=50,
        flush_interval=20,
        queue_size=1000
    )
    
    pipeline = UnifiedPipeline(
        operators=[ TranslationOperator(), 
            FilterOperator(min_len=50),
            WordCountOperator(),
             SegmentOperator()],
        input_uri=f"jsonl://{input_file}",
        output_uri="jsonl://tmp/demo/case9_final.jsonl",
        writer_config=writer_cfg,
        parallel_size=10,
        batch_size=5,
        streaming=True
    )
    pipeline.create(pipeline_id="case9_async_stress")
    
    start = time.time()
    pipeline.run()
    print(f"âœ“ åœºæ™¯ 9 å®Œæˆã€‚å¼‚æ­¥åˆ·ç›˜æ¨¡å¼ä¸‹è€—æ—¶: {time.time()-start:.2f}s")
    
    # éªŒè¯æ•°æ®å®Œæ•´æ€§
    output_path = "tmp/demo/case9_final.jsonl"
    if os.path.exists(output_path):
        with open(output_path, 'r', encoding='utf-8') as f:
            count = sum(1 for _ in f)
            print(f"ğŸ“Š æ•°æ®æ ¡éªŒ: è¾“å…¥ 500 æ¡ -> è¾“å‡º {count} æ¡ {'[PASS]' if count==500 else '[FAIL]'}")

# ========== æ–°åœºæ™¯ 10: æµå¼å¯é æ€§æµ‹è¯• (é’ˆå¯¹æ—©äº§ EOF / å»¶è¿Ÿå¯åŠ¨) ==========
def run_case_10():
    print("\n>>> åœºæ™¯ 10: æµå¼å¯é æ€§æµ‹è¯• (é’ˆå¯¹æ—©äº§ EOF / å»¶è¿Ÿå¯åŠ¨)")
    input_file = "tmp/demo/case10_input.jsonl"
    setup_data(input_file, count=20)
    
    class SlowStartupOperator(FunctionOperator):
        """æ¨¡æ‹Ÿä¸€ä¸ªå¯åŠ¨ææ…¢çš„ä¸Šæ¸¸ï¼Œè¯±å‘ä¸‹æ¸¸æ—©äº§ EOF"""
        def __init__(self):
            def slow_func(item):
                # ç¬¬ä¸€æ¡æ•°æ®æ•…æ„å»¶è¿Ÿå¾ˆä¹…æ‰äº§å‡º
                if item.get("id") == 0:
                    time.sleep(2.0) 
                return item
            super().__init__(func=slow_func)

    pipeline = UnifiedPipeline(
        operators=[SlowStartupOperator(), WordCountOperator()],
        input_uri=f"jsonl://{input_file}",
        output_uri="jsonl://tmp/demo/case10_final.jsonl",
        streaming=True, # å¼€å¯å¹¶è¡Œæµ
        batch_size=1
    )
    pipeline.create(pipeline_id="case10_eof_robustness")
    
    print("ğŸš€ å¯åŠ¨æµå¼é“¾è·¯ï¼Œè§‚æµ‹ä¸‹æ¸¸æ˜¯å¦èƒ½ç¨³å¥ç­‰å¾…ä¸Šæ¸¸æ•°æ® (é¢„æœŸä¼šæœ‰é‡è¯•æ—¥å¿—)...")
    pipeline.run()
    
    output_path = "tmp/demo/case10_final.jsonl"
    if os.path.exists(output_path):
        with open(output_path, 'r', encoding='utf-8') as f:
            count = sum(1 for _ in f)
            print(f"ğŸ“Š å¯é æ€§æ ¡éªŒ: è¾“å‡º {count}/20 æ¡ {'[PASS]' if count==20 else '[FAIL]'}")

# ========== æ–°åœºæ™¯ 11: æå°èƒŒå‹æµ‹è¯• (Small Queue Blocking) ==========
def run_case_11():
    print("\n>>> åœºæ™¯ 11: æå°å¼‚æ­¥é˜Ÿåˆ—èƒŒå‹æµ‹è¯• (éªŒè¯ç”Ÿäº§è€…é˜»å¡)")
    input_file = "tmp/demo/case11_input.jsonl"
    setup_data(input_file, count=100)
    
    # è®¾ç½®æå°çš„é˜Ÿåˆ—å¤§å°ï¼Œå¼ºåˆ¶è§¦å‘ç”Ÿäº§è€…é˜»å¡
    writer_cfg = WriterConfig(
        async_mode=True,
        queue_size=2,          # æå°é˜Ÿåˆ—
        flush_batch_size=10,   # æ”’å¤Ÿ 10 æ¡æ‰åˆ·
        flush_interval=5.0     # ä¸”åˆ·ç›˜æ—¶é—´é—´éš”å¾ˆé•¿
    )
    
    pipeline = UnifiedPipeline(
        operators=[TranslationOperator()],
        input_uri=f"jsonl://{input_file}",
        output_uri="jsonl://tmp/demo/case11_final.jsonl",
        writer_config=writer_cfg,
        parallel_size=20, # é«˜å¹¶å‘äº§ç”Ÿæ•°æ®
        batch_size=1
    )
    pipeline.create(pipeline_id="case11_backpressure")
    
    print("ğŸš€ å¯åŠ¨æµ‹è¯•ï¼Œè§‚å¯Ÿåœ¨ç£ç›˜åˆ·ç›˜å‰ï¼Œç”Ÿäº§è€…æ˜¯å¦ä¼šè¢« queue.put é˜»å¡...")
    start = time.time()
    pipeline.run()
    print(f"âœ“ åœºæ™¯ 11 å®Œæˆï¼Œè€—æ—¶: {time.time()-start:.2f}s")
    
    output_path = "tmp/demo/case11_final.jsonl"
    if os.path.exists(output_path):
        with open(output_path, 'r', encoding='utf-8') as f:
            count = sum(1 for _ in f)
            print(f"ğŸ“Š æ•°æ®æ ¡éªŒ: è¾“å‡º {count}/100 æ¡ {'[PASS]' if count==100 else '[FAIL]'}")

# ========== æ–°åœºæ™¯ 12: ç©ºè¾“å…¥æµ‹è¯• (Empty Input Edge Case) ==========
def run_case_12():
    print("\n>>> åœºæ™¯ 12: ç©ºè¾“å…¥è¾¹ç•Œæµ‹è¯•")
    input_file = "tmp/demo/case12_empty.jsonl"
    setup_data(input_file, count=0) # åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶
    
    pipeline = UnifiedPipeline(
        operators=[TranslationOperator(), WordCountOperator()],
        input_uri=f"jsonl://{input_file}",
        output_uri="jsonl://tmp/demo/case12_final.jsonl",
        streaming=True
    )
    pipeline.create(pipeline_id="case12_empty")
    
    print("ğŸš€ è¿è¡Œç©ºä»»åŠ¡...")
    pipeline.run()
    print("âœ“ åœºæ™¯ 12 å®Œæˆ (åº”æ— ä»»ä½•æŠ¥é”™)ã€‚")

# ========== æ–°åœºæ™¯ 13: CSV åè®®æµ‹è¯• (CSV Protocol Test) ==========
def run_case_13():
    print("\n>>> åœºæ™¯ 13: CSV åè®®æµ‹è¯• (CSV Protocol)")
    input_file = "tmp/demo/case13_input.csv"
    # å‡†å¤‡ CSV æ•°æ®
    os.makedirs(os.path.dirname(os.path.abspath(input_file)), exist_ok=True)
    with open(input_file, 'w', encoding='utf-8', newline='') as f:
        import csv
        writer = csv.DictWriter(f, fieldnames=["id", "text"])
        writer.writeheader()
        writer.writerow({"id": 0, "text": "Hello CSV"})
        writer.writerow({"id": 1, "text": "DataGen is powerful"})
    
    pipeline = UnifiedPipeline(
        operators=[TranslationOperator()],
        input_uri=f"csv://{input_file}",
        output_uri="csv://tmp/demo/case13_final.csv",
        streaming=False
    )
    pipeline.create(pipeline_id="case13_csv")
    
    print("ğŸš€ è¿è¡Œ CSV ä»»åŠ¡...")
    pipeline.run()
    
    # output_path = "tmp/demo/case13_final.csv"
    # if os.path.exists(output_path):
    #     with open(output_path, 'r', encoding='utf-8') as f:
    #         lines = f.readlines()
    #         print(f"ğŸ“Š CSV æ ¡éªŒ: è¾“å‡ºè¡Œæ•°={len(lines)} (å«è¡¨å¤´) {'[PASS]' if len(lines)==3 else '[FAIL]'}")
    #         print(f"ğŸ“„ æ ·ä¾‹å†…å®¹: {lines[-1].strip()}")

if __name__ == "__main__":
    setup_llm_model()
    
    # run_case_1()
    # run_case_2()
    # run_case_3()
    # run_case_4()
    # run_case_5()
    # run_case_6()
    # run_case_7()
    # run_case_8()
    run_case_9()
    run_case_10()
    run_case_11()
    run_case_12()
    run_case_13()