# llm-datagen 架构与实战百科 (QUESTION.md)

本手册汇聚了 llm-datagen 开发过程中的核心架构决策、曾踩过的深坑以及生产环境的最佳实践。分为 **底层开发篇**、**血泪教训篇** 和 **应用实战篇**。

---

## 第一部分：底层开发篇 (Framework Architecture)

### Q1: 为什么要坚持“一切皆流 (Everything is a Stream)”而废弃 `is_streaming` 标志位？
*   **现象**：过去需要手动判断节点是“批处理”还是“流处理”，导致逻辑支离破碎。
*   **设计决策**：引入“封条机制”。`Reader` 唯一的准则是：**只要桶里没水且没看到 `.done` 封条，就原地等待。**
*   **价值**：
    *   **静态文件**：被视为“启动即贴封条的流”。
    *   **动态流**：被视为“永不贴封条的流”。
    *   代码实现上彻底抹平了两者的差异，增强了跨 Pipeline 场景的组件复用性。

### Q2: 什么是“两阶段焊接 (Plan -> Weld)”流程？
*   **痛点**：早期版本中，路径推导依赖运行时 ID，导致出现了大量的 `None/` 文件夹或路径漂移。
*   **设计决策** (`DETAIL.md 7.4`)：
    1.  **Plan (规划期)**：先计算逻辑拓扑，锁定所有节点的 `node_id`。
    2.  **Weld (焊接期)**：根据确定的 ID 拼接物理 URI，并立即实例化物理对象（Digital Twin）。
*   **价值**：确保了 Pipeline 在 `run()` 之前就已经有了确定的物理结构，生成的物理蓝图立即落盘，保证了断点恢复的**确定性 (Determinism)**。

### Q3: 为什么进度汇报要改用“抢占式 (Pre-emptive Reporting)”？
*   **问题**：崩溃恢复后数据量减少（At-Most-Once 语义）。
*   **根源**：为了保护昂贵的算子（如 LLM），采用“取走即存盘”策略。一旦数据进入内存即视为消耗，防止重复扣费。
*   **策略**：将汇报时机放在“读取后、处理前”。虽然会导致崩溃瞬间的批次丢失，但确保了绝不重复执行昂贵操作。

### Q4: 如何处理 1:N 爆炸分发模式下的 ID 一致性？
*   **逻辑** (`DETAIL.md 465-479`)：
    *   **_i (物理真理)**：由框架维护的行号/锚点，用于断点续传。
    *   **衍生 ID**：采用公式 `unique_idx = parent_idx * 10000 + sub_idx`。
*   **价值**：确保即使发生爆炸分发，每一条衍生数据在物理文件中也有唯一的、可预测的锚点。

### Q13: I/O 总线如何实现“协议中立”与“路径自愈”？
*   **设计决策**：引入 `UnifiedFileStream` 代理。
*   **协议分发**：根据 URI 前缀（`jsonl://`, `file://`, `csv://`）自动实例化底层的物理驱动，开发者只需面对统一接口。
*   **路径自愈**：如果用户提供的路径缺失后缀（如 `file://data/tmp`），系统会根据协议自动补全（如 `.jsonl`），确保物理文件的合规性。

### Q14: 如何保证跨协议环境下的路径安全？
*   **痛点**：在手动拼接 `protocol_prefix` 和 `base_path` 时，极易出现 `file://path//subpath`（双斜杠）或丢失斜杠导致的路径解析错误。
*   **解决方案**：在 `BaseStream` 的 `uri` 和 `path` 推导中，强制执行 `rstrip("/")` 和 `lstrip("/")` 处理。
*   **价值**：确保了无论用户输入的路径是否规范，框架生成的物理路径始终是绝对合法的。

### Q15: 什么是“包络扁平化”，它解决了什么问题？
*   **现象**：早期版本由于 Reader 和 Writer 的多层包装，物理文件会出现 `{"data": {"data": {...}}}` 的嵌套现象（套娃）。
*   **解决方案**：`GenericWriter` 实施扁平化策略。在写入前自动嗅探数据包，确保物理存储层只有一层固定的包络 `{"_i": ..., "data": ...}`。
*   **价值**：保持了物理数据的纯净性，极大地方便了外部工具直接读取中间文件。

### Q16: 如何在多线程高并发写入时避免磁盘 I/O 成为瓶颈？
*   **痛点**：大量工作线程竞争同一个物理文件句柄，导致剧烈的 Context Switch 和 CPU 空转，甚至由于竞争产生“僵尸线程”，系统响应极慢。
*   **解决方案**：引入“异步批次写入器 (Asynchronous Batch Writer)”。
    *   **架构解耦**：将“数据生成”与“物理写入”物理隔离。工作线程执行内存操作后立即返回，不再阻塞等待磁盘 IO。
    *   **写汇聚 (I/O Merging)**：后台线程收集散碎结果，通过一次性 `storage.append_batch()` 执行大块连续写入，效率提升数倍。
*   **危险预防**：消除了高并发下的死锁风险，确保即使 `parallel_size` 设为 100，系统响应依然丝滑。

---

## 第二部分：血泪教训篇 (The Ghost in the Machine)

### Q5: 什么是“死锁幽灵 (The Deadlock Ghost)”？
*   **现象**：Pipeline 处理完最后一行数据后挂起，不退出。
*   **根源**：**信号丢失**。上游节点虽然运行结束，但没有调用 `close()`，导致下游节点一直在等待。
*   **教训**：Node 结束时必须强制执行 `writer.close()`。手动构建 Pipeline 时，起始源文件必须显式调用 `in_s.seal()` 发送物理封条。

### Q6: 什么是“位点幻读 (Offset Phantom Read)”？
*   **现象**：断点恢复后，数据输出翻倍（如 20 条输入产生了 34 条输出）。
*   **根源**：恢复启动时，输出文件采用 Append 模式，但之前的“脏数据”（由于崩溃未成功记录位点产生的冗余）没有被物理撤销。
*   **解决方案**：**物理截断 (Atomic Truncation)**。恢复时必须根据 Checkpoint 记录，物理强制裁掉输出文件中超出部分的行，并强制将 `Reader` 指针 Seek 到 `completed_count` 位置。

### Q7: 什么是“竞态幽灵 (The Race Ghost)”？
*   **现象** (`DETAIL.md 7.3`)：流式恢复时，下游节点进度显示 `10/10` 且不再更新，实际上游还有新数据。
*   **根源**：下游节点抢跑，看到了上一次运行留下的 `.done` 封条，误以为任务结束。
*   **解决方案**：
    1.  **同步预热**：在启动并发线程池前，按拓扑顺序同步执行所有节点的 `open()`。
    2.  **确权机制**：确保上游的 `unseal()`（撕掉封条）动作在物理时间上绝对领先。

### Q17: 如何解决并行节点 (ParallelBatchNode) 的“进度虚高”问题？
*   **现象**：高并发模式下进度条瞬间跳满，但实际后台还在处理，若崩溃会导致已汇报的数据由于未实际落盘严重丢失。
*   **解决方案**：引入“双轨进度”机制。
    *   **调度位点 (Checkpoint)**：任务派发瞬间更新，确保“最多一次 (At-most-once)”语义，保护昂贵算子不重跑。
    *   **感知位点 (UI/Hooks)**：任务真正写入成功后才回调汇报，确保用户看到的进度是物理真实的。

### Q18: 什么是“早产 EOF (Premature EOF)”，如何防御？
*   **现象**：流式 Pipeline 启动后下游瞬间检测到结束信号并退出，导致结果空洞或缺失。
*   **根源**：信号反超。下游探测速度快于上游产生第一条数据的速度，或者误读了残余的旧封条。
*   **加固方案** (`StreamBridge`)：
    1.  **零进度重试**：若在进度为 0 时检测到 EOF，强制执行退火重试（Retries），给予上游充分的预热时间。
    2.  **信号宽限期**：判定结束后给予微小延迟（Grace Period），确保异步写入缓冲区完成最后的物理落盘。

### Q19: 如何保证算子 (Operator) 的多态性与调用健壮性？
*   **痛点**：`UnifiedOperatorPipeline` 接收 `IOperator`，但底层节点往往需要区分 `IBatchOperator` (高效) 和 `ISingleOperator` (易用)，容易产生类型不匹配或调用异常。
*   **解决方案**：
    1.  **自动适配容器**：引入 `OperatorNode` 和 `ParallelOperatorNode`。它们不再强制要求算子类型，而是在运行时通过 `hasattr` 探测 `process_batch` 或 `process_item`。
    2.  **基类退火实现**：提供 `BaseOperator` 基类。如果开发者只实现了 `process_item`，基类会自动将其封装为高效的 `process_batch` 循环，反之亦然。
*   **价值**：极大增强了框架对第三方算子的包容性，开发者只需关注业务逻辑，无需担心底层容器的匹配问题。

### Q20: 为什么并行模式下崩溃，丢失的数据量远超 batch_size？（丢失放大）
*   **现象**：设置 `batch_size=10, parallel_size=5`，崩溃一次可能丢掉 50 条数据。
*   **根源**：在“最多一次 (At-most-once)”语义下，主线程为了保护昂贵算子，采取**派发即消耗**策略。
    *   主线程连续抓取 5 个批次塞进线程池，此时磁盘 Checkpoint 已跳过 50 条。
    *   如果其中一个线程崩溃导致进程退出，其他 4 个正在内存中处理的批次由于尚未落盘，且位点已过，将永远丢失。
*   **权衡**：这是为了防止昂贵 Token 重复消耗而付出的必要代价。若要减少丢失，请调小 `batch_size`。

### Q21: `_i` (物理 ID) 和 `id` (业务 ID) 我该选哪个？
*   **_i (真理)**：框架自动生成的物理行号，是断点续传的**唯一锚点**。不可修改，恢复时框架只看它。
*   **id (逻辑)**：用户的业务主键。框架不感知，仅透传。
*   **最佳实践**：在 1:N 爆炸分发时，回溯父节点应优先用业务 `id`；若怕业务 ID 缺失，用 `_i` 做物理保底。

### Q22: 任务不结束/进度异常的快速排查清单
1.  **不结束**：检查是否漏掉了 `writer.close()` 或 `stream.seal()`（特别是手动构建节点时）。
2.  **进度不足**：通常是因为“最多一次”语义在崩溃时丢弃了内存中的批次。
3.  **进度超出**：检查是否在恢复时错误地将 `Writer` 设为了覆盖模式而非追加模式。
4.  **无感恢复失败**：检查 `runtime.json` 是否正确持久化了 `streaming` 标志位。

---

## 第三部分：应用实战篇 (Usage & Production)

### Q8: 为什么我设置了 `recoverable=True` 却报错 `BusError`？
*   **核心原则**：**报错优于隐式纠正**。
*   **原因**：如果你启用了“可恢复性”，但 `default_protocol` 设为了 `memory://`（内存不可持久化），系统严禁进行“暗箱纠正”。
*   **建议**：必须改用 `file://` 或其他支持 Seek 寻址的协议。

### Q9: 为什么使用 `writer.get_written_count()` 作为检查点真理？
*   **对比** (`DESIGN_EXPERIENCE.md 267-304`)：
    *   **方案 A (写入量)**：结果可能导致重复处理（浪费算力），但有去重保护，数据不重不丢。
    *   **方案 B (读取量)**：可能导致数据永久丢失（不可接受）。
*   **结论**：数据完整性优先。尽管会有少量重复处理风险，但这确保了数据不丢。

### Q10: 如何防止大规模处理时的 OOM（内存溢出）？
*   **核心矛盾**：上游读取极快（毫秒级），下游 LLM 处理极慢（秒级）。若无干预，百万级数据会瞬间堆积在内存中。
*   **双重背压解决方案**：
    1.  **算子级背压**：在 `ParallelBatchNode` 中，利用 `Semaphore` 限制派发中的任务数。当线程池满载时，主线程停止从 Reader 抓取新数据。
    2.  **传输级背压 (New)**：在异步写入器中设置 `queue_size` 硬上限。当写入速度跟不上处理速度导致队列满员时，`queue.put()` 会直接阻塞工作线程，产生反向压力。
*   **价值**：确保内存占用始终处于恒定水平，不会随处理总量线性增长，彻底终结大规模任务下的 OOM 崩溃。

### Q11: 路径优先级 P1-P3 是如何运作的？
*   **P1 (最高)**：Node 显式指定的 URI（开发者手动配置）。
*   **P2 (次高)**：Pipeline 顶层参数（用于快速设置首尾）。
*   **P3 (最低)**：自动推导（中间链路根据 `pipeline_id` 自动焊接）。
*   **冲突原则**：严禁静默覆盖。如果首节点已手动指定了输入 A，Pipeline 参数却给了输入 B，系统将报错拦截。

### Q12: 为什么我的 Token 消耗统计显示为 0？
*   **根源**：**全量推送模型 (The Push Model)**。如果 `Operator` 没有通过 `ctx.report_usage(metrics)` 推送数据，外部 Hooks 就无法感知。
*   **原则**：外部 Hooks 永远不主动“抓取”Node 属性，而是依赖 Node 主动推送，避免线程竞争和状态滞后。

---

### 核心设计哲学对照表 (Summary)

| 维度 | 文档准则 | 违反后果 |
| :--- | :--- | :--- |
| **Node 角色** | 执行容器 (躯干)，Operator 是插件 (灵魂) | 逻辑耦合，难以测试 |
| **Bus 角色** | 一切皆流 (Everything is a Stream) | 静态/动态逻辑分裂，代码冗余 |
| **恢复原则** | 磁盘真理优先 (Disk Truth First) | 产生位点幻读、数据丢失或翻倍 |
| **汇报语义** | 最多一次 (At-most-once) 优先 | 昂贵 Token 被重复消耗 |
| **存储管理** | 协议中立 (Protocol-Neutral) | 换环境（如本地到 OSS）需重写大量代码 |
